{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfead40",
   "metadata": {},
   "source": [
    "# Parallel Corpora and LMs\n",
    "\n",
    "You receive a small parallel corpus extracted from EuroParl, a very influential corpus in Machine Translation which contains speeches delivered at the European Parliament translated in a variety of languages: you'll work with English, Dutch, and Italian. The corpus comes as a .json file containing a dictionary mapping IDs to sentences in the three target languages, indicated as 'en', 'it', and 'nl'. Sentences under each language come as a list of strings, and sentences have been tokenized already, with tokens separated by white spaces.\n",
    "\n",
    "I've resolved some incongruencies from the first attempt that were identified in the discussion board, so make sure to re-read the assignment carefully and to respect naming conventions: there are examples to guide you, and if in doubt ask on the discussion board.\n",
    "\n",
    "I've also decided to eliminate the sub-task on splitting training and test set: nearly everybody completed it correctly, so it wasn't really making a difference and was introducing potential sources of divergence in the results because multiple splitting procedures were possible given the specification. You now get two files, one for training and one for testing, that you should pre-process as indicated. The 1 point awarded for correctly splitting the original corpus is awarded by default to everybody, so the assignment is still out of 40 points and everybody starts with 1 point.\n",
    "\n",
    "You should carry out the following tasks:\n",
    "\n",
    "1. read the input .json files for training and test, then:\n",
    "\t- pre-process sentences in all three languages by \n",
    "        * lowercasing everything (1pt)\n",
    "        * replacing each digit with the capital letter D (1pt)\n",
    "        * removing all characters that aren't letters (mind all sorts of special characters!) or white spaces (2pts)\n",
    "    \n",
    "> 4 points available, assigned as indicated above if the step is carried out correctly (everything is lowercased, all numbers replaced, only letters and white spaces). \n",
    "\n",
    "\n",
    "2. train a total of four character-level Statistical Language Models using the LM class provided in Notebook04 (make sure the resulting object is of class LM and has attributes _counts_, _vocab_, and _vocab\\_size_), with add-k smoothing and k=0.01:\n",
    "\t- a model predicting the current character based on the two previous characters\n",
    "\t- a model predicting the current character based on the four previous characters\n",
    "\n",
    "  given the following inputs:\n",
    "    - the English sentences in the training set, after getting rid of all white spaces\n",
    "    - the word types extracted from the English sentences in the training set\n",
    "\n",
    "\n",
    "\n",
    "- !!! Replace any character which occurs fewer than 20 times in the English sentences from the training set with the string '?'. \n",
    "- !!! Remember that language models can only be compared if they have the same vocabulary: make sure that all models are trained using the vocabulary of the models trained on English sentences, not word types.\n",
    "- !!! Get inspiration from the Corpus and LM classes introduced in class, but edit them to fit the task.\n",
    "- !!! remember to set BoS and EoS correctly.\n",
    "    \n",
    "At the end of task 2 you should have four LMs all having the same vocabulary:\n",
    "* a character-level language model trained on full English sentences without white spaces to predict the next character given the two preceding ones\n",
    "* a character-level language model trained on word types from the English training sentences to predict the next character given the two preceding ones\n",
    "* a character-level language model trained on full English sentences without white spaces to predict the next character given the four preceding ones\n",
    "* a character-level language model trained on word types from the English training sentences to predict the next character given the four preceding ones\n",
    "\n",
    "You should submit a .pkl file for each LM, dumping the LMs to .pkl files and naming files using the template Name(Initial)Surname\\_[words|sents]\\_[2gr|4gr]\\_en.pkl ( the | symbol means OR ). Therefore, John K. Doe should name his model trained on sentences and predicting based on two preceding characters JohnKDoe\\_sents\\_2gr\\_en.pkl. If you don't have a middle name, just use NameSurname. If you have multiple surnames, add them as NameSurname1Surname2, with no intervening spaces. The notebook contains the code backbone to save a .pkl file, you need to edit it to choose the correct object to save and the appropriate file name given your name and surname.\n",
    "\n",
    "> 4 points available: we will automatically check whether 5 random transition counts and the vocabulary of your models check out with ours. For each model where the check succeeds, you will receive one point.\n",
    "\n",
    "\n",
    "3. Compute the perplexity of all four Language Models on:\n",
    "\t- the English sentences from the training set\n",
    "\t- the English word types from the training set\n",
    "\t- the English sentences from the test set\n",
    "\t- the Dutch sentences from the test set\n",
    "\t- the Italian sentences from the test set\n",
    "\n",
    "You should submit a .csv file with the following structure, column names, and values ([2/4] means either 2 for LMs predicting based on two previous characters or 4 for LMs predicting based on four previous characters, the options under test_data indicate the five sets to be used to compute perplexity):\n",
    "\n",
    "|ngram_size|training_data|test_data|perplexity|\n",
    "|---|---|---|---|\n",
    "|[2/4]|[words/sents]|[ITtest/NLtest/ENtest/ENtrain\\_sents/ENtrain\\_words]|float (rounded at 4 decimal places)|\n",
    "|---|---|---|---|\n",
    "\n",
    "The file should be named according to the template Name(Initial)Surname\\_perplexities.csv\n",
    "\n",
    "> 5 points available: you get 1 point if all four LMs yield the correct perplexity scores for a test_dataset.\n",
    "\n",
    "\n",
    "4. Out of all Italian and Dutch word types in the test sentences, restricting attention to word types consisting of at least 5 characters and with at least 5 occurrences in the Italian/Dutch test sentences, find:\n",
    "\t- the word in each language with the lowest perplexity according to each of the four LMs\n",
    "\t- the word in each language with the highest perplexity according to each of four LMs\n",
    "\n",
    "You should submit two .csv files (one for the lowest perplexities, one for highest perplexities) with the following structure, column names, and values ([it/nl] indicates the language, with it indicating italian and nl indicating dutch, str indicates that the word should appear as a string, [2/4] means either 2 for LMs predicting based on two previous characters or 4 for LMs predicting based on four previous characters, [words/sents] indicates whether the model identifying that particular word on that language was trained on word types or sentences):\n",
    "\n",
    "| lang | word | ngram_size | training_data | perplexity |\n",
    "|---|---|---|---|---|\n",
    "|[it/nl]|str|[2/4]|[words/sents]|float (rounded at 4 decimal places)|\n",
    "|---|---|---|---|\n",
    "\n",
    "The files should be named according to the template Name(Initial)Surname\\_perplexities\\_[max|min].csv, so Jane Smith should submit a file named JaneSmith\\_perplexities\\_max.csv containing 8 rows each storing the word with the highest perplexity according to each of the four LMs per language.\n",
    "\n",
    "> 4 points available: you get 0.25 points for each correct word identified\n",
    "\n",
    "\n",
    "5. Answer the following questions:\n",
    "\t- a. compare LMs' perplexity on the English training sets, sentences and words, then explain the differences in perplexity considering what changes between the two training set-ups. (5 pts, 150 words)\n",
    "\t- b. which LM trained on sentences generalizes better to unseen sentences in the same language, bigram or tetragram? explain why this is the case. (5 pts, 150 words)\n",
    "\t- c. compare LMs trained on English in their ability to fit Italian and Dutch sentences: which factor between ngram size and training corpus (words or sentences) affects perplexity the most? Explain why we observe this pattern. (4 pts, 100 words)\n",
    "\t- d. what patterns can you identify in the words with the lowest perplexity in Dutch and Italian? (4 pts, 100 words)\n",
    "\t- e. what patterns can you identify in the words with the highest perplexity in Dutch and Italian? (4 pts, 100 words)\n",
    "    \n",
    "> 22 points in total, see specifications next to each question\n",
    "\n",
    "\n",
    "Summing up, you will have to submit 8 files:\n",
    "- 1 python notebook in .ipynb format - name the file as \\Name(Initial)Surname\\_CLassignment.ipynb\n",
    "- 4 .pkl files each containing an LM object with attributes _counts_, _vocab_, and _vocab\\_size_ (you can of course add multiple attributes if it helps you, but these three have to be there, with those exact names!)\n",
    "- 3 .csv files, one storing the perplexity for each model on the five possible test sets (Italian, Dutch, English sentences from the test set; English sentences from training set, and English word types from the training set); one storing the words in Dutch and Italian with the highest perplexity according to each LM; one storing the words in Dutch and Italian with the lowest perplexity according to each LM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a274bd",
   "metadata": {},
   "source": [
    "## Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a802c",
   "metadata": {},
   "source": [
    "1. read the input .json files for training and test, then:\n",
    "\t- pre-process sentences in all three languages by \n",
    "        * lowercasing everything (1pt)\n",
    "        * replacing each digit with the capital letter D (1pt)\n",
    "        * removing all characters that aren't letters (mind all sorts of special characters!) or white spaces (2pts)\n",
    "    \n",
    "> 4 points available, assigned as indicated above if the step is carried out correctly (everything is lowercased, all numbers replaced, only letters and white spaces). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c603a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle as pkl\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df11744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training corpus size: 6117\n",
      "Testing corpus size: 1529\n",
      "Example preprocessed sentence (train corpus):\n",
      "en: member of the commission   i agree with your way of reasoning and i think it is essential that we see the euro as a key policy instrument for economic policy and sustainable growth in europe and  at the same time  that we look at it not only as a symbol but also as a bond for europeans in building the common european home \n",
      "nl: lid van de commissie    en  ik ben het eens met uw redenering  en ik denk dat het essentieel is dat we de euro zien als een onmisbaar beleidsinstrument voor economisch beleid en duurzame groei in europa en dat we hem tegelijkertijd niet alleen zien als een symbool  maar ook als een verbindende factor voor europeanen bij het bouwen van ons gemeenschappelijk europees thuis \n",
      "it: signor presidente  concordo con tale ragionamento e penso che sia essenziale vedere l euro come strumento politico essenziale per la politica economica e la crescita sostenibile in europa e  nel contempo  considerarlo non soltanto un simbolo  bensì anche un legante per gli europei nella costruzione della casa comune europea \n"
     ]
    }
   ],
   "source": [
    "# Define the file paths for training and testing datasets\n",
    "train_file_path = '/Users/belizpekkan/Desktop/Programming/Computational Linguistics/Assignment Submission 2/training_parallel_sentences_en-nl-it.json'\n",
    "test_file_path = '/Users/belizpekkan/Desktop/Programming/Computational Linguistics/Assignment Submission 2/test_parallel_sentences_en-nl-it.json'\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase \n",
    "    text = re.sub(r'\\d', 'D', text)  # Replace each digit with 'D'    \n",
    "    text = re.sub(r'[^a-zàèéìòóùëïöüáí\\s]', '', text) # Remove non-letter characters except accented ones and white spaces\n",
    "    return text\n",
    "\n",
    "def preprocess_sentences(sentences):\n",
    "    return [preprocess_text(sentence) for sentence in sentences]\n",
    "\n",
    "def load_and_preprocess_corpus(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        corpus = json.load(file)\n",
    "\n",
    "    for sentence_id in corpus:\n",
    "        for lang in corpus[sentence_id]:\n",
    "            corpus[sentence_id][lang] = preprocess_sentences(corpus[sentence_id][lang])\n",
    "    return corpus\n",
    "\n",
    "def replace_infrequent_chars(sentences, threshold=20):\n",
    "    all_chars = [char for sentence in sentences for char in sentence]\n",
    "    char_freq = Counter(all_chars)\n",
    "    frequent_chars = {char for char, freq in char_freq.items() if freq >= threshold}\n",
    "    processed_sentences = [\n",
    "        ''.join([char if char in frequent_chars else '?' for char in sentence])\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "    return processed_sentences, frequent_chars\n",
    "\n",
    "# Load and preprocess the training and testing datasets\n",
    "train_corpus = load_and_preprocess_corpus(train_file_path)\n",
    "test_corpus = load_and_preprocess_corpus(test_file_path)\n",
    "\n",
    "# Output the size of the corpora\n",
    "print(f\"Training corpus size: {len(train_corpus)}\")\n",
    "print(f\"Testing corpus size: {len(test_corpus)}\")\n",
    "\n",
    "# Example to print a preprocessed sentence from the training corpus\n",
    "example_sentence_id = next(iter(train_corpus))\n",
    "print(f\"Example preprocessed sentence (train corpus):\")\n",
    "for lang, sentences in train_corpus[example_sentence_id].items():\n",
    "    print(f\"{lang}: {sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091e693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "english_test_sentences = [''.join(test_corpus[sentence_id]['en']) for sentence_id in test_corpus]\n",
    "english_test_sentences_no_spaces = [list(sentence.replace(' ', '')) for sentence in english_test_sentences]\n",
    "\n",
    "dutch_test_sentences = [''.join(test_corpus[sentence_id]['nl']) for sentence_id in test_corpus]\n",
    "dutch_test_sentences_no_spaces = [list(sentence.replace(' ', '')) for sentence in dutch_test_sentences]\n",
    "\n",
    "italian_test_sentences = [''.join(test_corpus[sentence_id]['it']) for sentence_id in test_corpus]\n",
    "italian_test_sentences_no_spaces = [list(sentence.replace(' ', '')) for sentence in italian_test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502f6a9",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e721e",
   "metadata": {},
   "source": [
    "2. train a total of four character-level Statistical Language Models using the LM class provided in Notebook04 (make sure the resulting object is of class LM and has attributes _counts_, _vocab_, and _vocab\\_size_), with add-k smoothing and k=0.01:\n",
    "\t- a model predicting the current character based on the two previous characters\n",
    "\t- a model predicting the current character based on the four previous characters\n",
    "\n",
    "  given the following inputs:\n",
    "    - the English sentences in the training set, after getting rid of all white spaces\n",
    "    - the word types extracted from the English sentences in the training set\n",
    "\n",
    "\n",
    "\n",
    "- !!! Replace any character which occurs fewer than 20 times in the English sentences from the training set with the string '?'. \n",
    "- !!! Remember that language models can only be compared if they have the same vocabulary: make sure that all models are trained using the vocabulary of the models trained on English sentences, not word types.\n",
    "- !!! Get inspiration from the Corpus and LM classes introduced in class, but edit them to fit the task.\n",
    "- !!! remember to set BoS and EoS correctly.\n",
    "    \n",
    "At the end of task 2 you should have four LMs all having the same vocabulary:\n",
    "* a character-level language model trained on full English sentences without white spaces to predict the next character given the two preceding ones\n",
    "* a character-level language model trained on word types from the English training sentences to predict the next character given the two preceding ones\n",
    "* a character-level language model trained on full English sentences without white spaces to predict the next character given the four preceding ones\n",
    "* a character-level language model trained on word types from the English training sentences to predict the next character given the four preceding ones\n",
    "\n",
    "You should submit a .pkl file for each LM, dumping the LMs to .pkl files and naming files using the template Name(Initial)Surname\\_[words|sents]\\_[2gr|4gr]\\_en.pkl ( the | symbol means OR ). Therefore, John K. Doe should name his model trained on sentences and predicting based on two preceding characters JohnKDoe\\_sents\\_2gr\\_en.pkl. If you don't have a middle name, just use NameSurname. If you have multiple surnames, add them as NameSurname1Surname2, with no intervening spaces. The notebook contains the code backbone to save a .pkl file, you need to edit it to choose the correct object to save and the appropriate file name given your name and surname.\n",
    "\n",
    "> 4 points available: we will automatically check whether 5 random transition counts and the vocabulary of your models check out with ours. For each model where the check succeeds, you will receive one point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b213e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, sentences, t=20, n=2, bos_eos=True):\n",
    "        self.sentences = sentences\n",
    "        self.t = t\n",
    "        self.ngram_size = n\n",
    "        self.bos_eos = bos_eos\n",
    "        \n",
    "        self.sentences, self.vocab = replace_infrequent_chars(self.sentences, self.t)\n",
    "        \n",
    "        if self.bos_eos:\n",
    "            self.sentences = self.add_bos_eos()\n",
    "\n",
    "    def add_bos_eos(self):\n",
    "        \"\"\"\n",
    "        Adds the necessary number of BOS symbols and one EOS symbol.\n",
    "        \"\"\"\n",
    "        r = self.ngram_size - 1\n",
    "        padded_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            padded_sentence = ['#bos#']*r + list(sentence) + ['#eos#']\n",
    "            padded_sentences.append(padded_sentence)\n",
    "        return padded_sentences\n",
    "\n",
    "# LM class for language model\n",
    "def default_int_dict():\n",
    "    return defaultdict(int)\n",
    "\n",
    "class LM:\n",
    "    def __init__(self, corpus_sentences, n, k=0.01):\n",
    "        self.ngram_size = n\n",
    "        self.k = k\n",
    "        \n",
    "        all_chars = [char for sentence in corpus_sentences for char in sentence]\n",
    "        char_freq = Counter(all_chars)\n",
    "        \n",
    "        frequent_chars = {char for char, freq in char_freq.items() if freq >= 20}\n",
    "        \n",
    "        processed_sentences = [\n",
    "            [char if char in frequent_chars else '?' for char in sentence]\n",
    "            for sentence in corpus_sentences\n",
    "        ]\n",
    "        \n",
    "        self.vocab = frequent_chars.union({'?'})\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.counts = defaultdict(default_int_dict)\n",
    "        self.update_counts(processed_sentences)\n",
    "\n",
    "    def update_counts(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            sentence = ['#bos#'] * (self.ngram_size - 1) + list(sentence) + ['#eos#']\n",
    "            for i in range(len(sentence) - self.ngram_size + 1):\n",
    "                ngram = tuple(sentence[i:i+self.ngram_size])\n",
    "                prefix = ngram[:-1]\n",
    "                char = ngram[-1]\n",
    "                self.counts[prefix][char] += 1\n",
    "\n",
    "    def calculate_probability(self, prefix, char):\n",
    "        prefix_counts = sum(self.counts[prefix].values()) + self.k * self.vocab_size\n",
    "        char_count = self.counts[prefix].get(char, 0) + self.k\n",
    "        return char_count / prefix_counts\n",
    "\n",
    "    def perplexity(self, sentences):\n",
    "        log_prob = 0\n",
    "        num_chars = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = ['#bos#'] * (self.ngram_size - 1) + list(sentence) + ['#eos#']\n",
    "            num_chars += len(sentence) - self.ngram_size + 1\n",
    "            for i in range(len(sentence) - self.ngram_size + 1):\n",
    "                ngram = tuple(sentence[i:i+self.ngram_size])\n",
    "                prefix = ngram[:-1]\n",
    "                char = ngram[-1]\n",
    "                prob = self.calculate_probability(prefix, char)\n",
    "                log_prob += np.log(prob)\n",
    "        \n",
    "        return np.exp(-log_prob / num_chars)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        with open(file_name, 'wb') as f_out:\n",
    "            pkl.dump(self, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05717dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'e', 'm', 'b', 'e', 'r', 'o', 'f', 't', 'h', 'e', 'c', 'o', 'm', 'm', 'i', 's', 's', 'i', 'o', 'n', 'i', 'a', 'g', 'r', 'e', 'e', 'w', 'i', 't', 'h', 'y', 'o', 'u', 'r', 'w', 'a', 'y', 'o', 'f', 'r', 'e', 'a', 's', 'o', 'n', 'i', 'n', 'g', 'a', 'n', 'd', 'i', 't', 'h', 'i', 'n', 'k', 'i', 't', 'i', 's', 'e', 's', 's', 'e', 'n', 't', 'i', 'a', 'l', 't', 'h', 'a', 't', 'w', 'e', 's', 'e', 'e', 't', 'h', 'e', 'e', 'u', 'r', 'o', 'a', 's', 'a', 'k', 'e', 'y', 'p', 'o', 'l', 'i', 'c', 'y', 'i', 'n', 's', 't', 'r', 'u', 'm', 'e', 'n', 't', 'f', 'o', 'r', 'e', 'c', 'o', 'n', 'o', 'm', 'i', 'c', 'p', 'o', 'l', 'i', 'c', 'y', 'a', 'n', 'd', 's', 'u', 's', 't', 'a', 'i', 'n', 'a', 'b', 'l', 'e', 'g', 'r', 'o', 'w', 't', 'h', 'i', 'n', 'e', 'u', 'r', 'o', 'p', 'e', 'a', 'n', 'd', 'a', 't', 't', 'h', 'e', 's', 'a', 'm', 'e', 't', 'i', 'm', 'e', 't', 'h', 'a', 't', 'w', 'e', 'l', 'o', 'o', 'k', 'a', 't', 'i', 't', 'n', 'o', 't', 'o', 'n', 'l', 'y', 'a', 's', 'a', 's', 'y', 'm', 'b', 'o', 'l', 'b', 'u', 't', 'a', 'l', 's', 'o', 'a', 's', 'a', 'b', 'o', 'n', 'd', 'f', 'o', 'r', 'e', 'u', 'r', 'o', 'p', 'e', 'a', 'n', 's', 'i', 'n', 'b', 'u', 'i', 'l', 'd', 'i', 'n', 'g', 't', 'h', 'e', 'c', 'o', 'm', 'm', 'o', 'n', 'e', 'u', 'r', 'o', 'p', 'e', 'a', 'n', 'h', 'o', 'm', 'e', 'i', 'n', 't', 'h', 'i', 's', 'r', 'e', 'g', 'a', 'r', 'd', 'w', 'e', 'h', 'a', 'v', 'e', 'o', 'r', 'g', 'a', 'n', 'i', 's', 'e', 'd', 'a', 'b', 'r', 'o', 'a', 'd', 'r', 'a', 'n', 'g', 'e', 'o', 'f', 'c', 'o', 'm', 'm', 'u', 'n', 'i', 'c', 'a', 't', 'i', 'o', 'n', 'a', 'c', 't', 'i', 'v', 'i', 't', 'i', 'e', 's', 'f', 'o', 'r', 'i', 'n', 's', 't', 'a', 'n', 'c', 'e', 'i', 'n', 't', 'h', 'e', 'f', 'i', 'r', 's', 't', 'h', 'a', 'l', 'f', 'o', 'f', 'i', 'w', 'i', 'l', 'l', 'j', 'u', 's', 't', 'g', 'i', 'v', 'e', 'y', 'o', 'u', 't', 'h', 'e', 'h', 'i', 'g', 'h', 'l', 'i', 'g', 'h', 't', 's', 'w', 'e', 'c', 'a', 'r', 'r', 'i', 'e', 'd', 'o', 'u', 't', 'a', 'n', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n', 'c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', 'o', 'n', 't', 'h', 'e', 'k', 'e', 'y', 'b', 'e', 'n', 'e', 'f', 'i', 't', 's', 'o', 'f', 't', 'h', 'e', 'e', 'u', 'r', 'o', 'i', 'n', 'e', 'u', 'r', 'o', 'a', 'r', 'e', 'a', 'c', 'o', 'u', 'n', 't', 'r', 'i', 'e', 's', 'g', 'e', 'r', 'm', 'a', 'n', 'y', 'f', 'r', 'a', 'n', 'c', 'e', 'i', 't', 'a', 'l', 'y', 'p', 'o', 'r', 't', 'u', 'g', 'a', 'l', 't', 'h', 'e', 'n', 'e', 't', 'h', 'e', 'r', 'l', 'a', 'n', 'd', 's', 's', 'p', 'a', 'i', 'n', 'a', 'u', 's', 't', 'r', 'i', 'a', 'f', 'i', 'n', 'l', 'a', 'n', 'd', 'm', 'a', 'l', 't', 'a', 'a', 'n', 'd', 'b', 'e', 'l', 'g', 'i', 'u', 'm', 'w', 'e', 'a', 'l', 's', 'o', 'o', 'r', 'g', 'a', 'n', 'i', 's', 'e', 'd', 't', 'h', 'e', 'b', 'r', 'u', 's', 's', 'e', 'l', 's', 'e', 'c', 'o', 'n', 'o', 'm', 'i', 'c', 'f', 'o', 'r', 'u', 'm', 'd', 'e', 'v', 'o', 't', 'e', 'd', 't', 'o', 'e', 'c', 'o', 'n', 'o', 'm', 'i', 'c', 'a', 'n', 'd', 'f', 'i', 'n', 'a', 'n', 'c', 'i', 'a', 'l', 'i', 's', 's', 'u', 'e', 's', 'w', 'h', 'i', 'c', 'h', 't', 'o', 'o', 'k', 'p', 'l', 'a', 'c', 'e', 't', 'w', 'o', 'w', 'e', 'e', 'k', 's', 'a', 'g', 'o', 'a', 'r', 'e', 'c', 'o', 'r', 'd', 'n', 'u', 'm', 'b', 'e', 'r', 'o', 'f', 'p', 'o', 'l', 'i', 'c', 'y', 'a', 'r', 'e', 'a', 's', 'm', 'o', 'r', 'e', 't', 'h', 'a', 'n', 'w', 'e', 'r', 'e', 'a', 'g', 'r', 'e', 'e', 'd', 'o', 'n', 'a', 'n', 'd', 't', 'h', 'e', 'e', 'v', 'e', 'n', 't', 'w', 'a', 's', 'w', 'i', 'd', 'e', 'l', 'y', 'r', 'e', 'p', 'o', 'r', 't', 'e', 'd', 'b', 'y', 't', 'h', 'e', 'm', 'e', 'd', 'i', 'a', 't', 'h', 'i', 's', 'w', 'a', 's', 'c', 'l', 'e', 'a', 'r', 'l', 'y', 'a', 'c', 'a', 's', 'e', 'o', 'f', 'r', 'e', 'a', 'c', 'h', 'i', 'n', 'g', 'o', 'u', 't', 't', 'o', 't', 'h', 'e', 'o', 'p', 'i', 'n', 'i', 'o', 'n', 'm', 'a', 'k', 'e', 'r', 's', 'a', 'n', 'd', 't', 'r', 'y', 'i', 'n', 'g', 't', 'o', 'h', 'a', 'v', 'e', 'a', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'i', 'e', 'r', 'e', 'f', 'f', 'e', 'c', 't', 't', 'h', 'r', 'o', 'u', 'g', 'h', 't', 'h', 'i', 's', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't', 'f', 'o', 'r', 'u', 'm', 'i', 'c', 'o', 'u', 'l', 'd', 'c', 'o', 'n', 't', 'i', 'n', 'u', 'e', 't', 'h', 'e', 'l', 'i', 's', 't', 'w', 'e', 'a', 'r', 'e', 'e', 'n', 'g', 'a', 'g', 'e', 'd', 'i', 'n', 't', 'h', 'e', 's', 'e', 'a', 'c', 't', 'i', 'v', 'i', 't', 'i', 'e', 's', 'a', 'n', 'd', 'i', 't', 'h', 'i', 'n', 'k', 'w', 'e', 'w', 'i', 'l', 'l', 'a', 'c', 'h', 'i', 'e', 'v', 'e', 't', 'h', 'e', 'b', 'e', 's', 't', 'r', 'e', 's', 'u', 'l', 't', 's', 'b', 'y', 'h', 'a', 'v', 'i', 'n', 'g', 't', 'h', 'e', 'r', 'i', 'g', 'h', 't', 'p', 'o', 'l', 'i', 'c', 'i', 'e', 's', 'a', 'n', 'd', 'h', 'a', 'v', 'i', 'n', 'g', 't', 'h', 'e', 'r', 'i', 'g', 'h', 't', 'k', 'i', 'n', 'd', 'o', 'f', 'c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', 's', 'i', 'n', 'o', 'r', 'd', 'e', 'r', 't', 'o', 'e', 'n', 's', 'u', 'r', 'e', 't', 'h', 'a', 't', 'o', 'u', 'r', 'p', 'e', 'o', 'p', 'l', 'e', 'a', 'r', 'e', 'w', 'e', 'l', 'l', 'i', 'n', 'f', 'o', 'r', 'm', 'e', 'd', 'i', 'n', 'm', 'y', 'y', 'o', 'u', 't', 'h', 'i', 't', 'r', 'a', 'i', 'n', 'e', 'd', 'a', 's', 'a', 's', 'p', 'a', 'r', 'e', 'p', 'a', 'r', 't', 's', 's', 'a', 'l', 'e', 's', 'm', 'a', 'n', 'i', 's', 't', 'a', 'r', 't', 'e', 'd', 'a', 't', 'a', 'n', 'd', 'i', 'e', 'n', 'd', 'e', 'd', 't', 'h', 'a', 't', 'c', 'a', 'r', 'e', 'e', 'r', 'a', 't', 'w', 'e', 'a', 'l', 'w', 'a', 'y', 's', 'h', 'a', 'd', 'a', 'v', 'e', 'r', 'y', 'c', 'l', 'e', 'a', 'r', 'p', 'h', 'i', 'l', 'o', 's', 'o', 'p', 'h', 'y', 't', 'h', 'a', 't', 'y', 'o', 'u', 'h', 'a', 'v', 'e', 't', 'o', 'h', 'a', 'v', 'e', 't', 'h', 'e', 'p', 'r', 'o', 'd', 'u', 'c', 't', 'r', 'i', 'g', 'h', 't', 'b', 'e', 'f', 'o', 'r', 'e', 'y', 'o', 'u', 'c', 'a', 'n', 'm', 'a', 'r', 'k', 'e', 't', 'i', 't', 'w', 'e', 'l', 'l', 'y', 'o', 'u', 'n', 'e', 'e', 'd', 'b', 'o', 't', 'h', 'y', 'o', 'u', 'n', 'e', 'e', 'd', 'a', 's', 'o', 'l', 'i', 'd', 'p', 'r', 'o', 'd', 'u', 'c', 't', 's', 'o', 'l', 'i', 'd', 'p', 'o', 'l', 'i', 'c', 'i', 'e', 's', 'a', 'n', 'd', 'y', 'o', 'u', 'n', 'e', 'e', 'd', 'a', 's', 'o', 'u', 'n', 'd', 'm', 'a', 'r', 'k', 'e', 't', 'i', 'n', 'g', 'c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', 'i', 'n', 'o', 'r', 'd', 'e', 'r', 't', 'o', 'w', 'i', 'n', 't', 'h', 'e', 'h', 'e', 'a', 'r', 't', 's', 'a', 'n', 'd', 'm', 'i', 'n', 'd', 's', 'o', 'f', 'o', 'u', 'r', 'p', 'e', 'o', 'p', 'l', 'e']\n",
      "[['m', 'e', 'm', 'b', 'e', 'r'], ['o', 'f'], ['t', 'h', 'e'], ['c', 'o', 'm', 'm', 'i', 's', 's', 'i', 'o', 'n'], ['i'], ['a', 'g', 'r', 'e', 'e'], ['w', 'i', 't', 'h'], ['y', 'o', 'u', 'r'], ['w', 'a', 'y'], ['o', 'f'], ['r', 'e', 'a', 's', 'o', 'n', 'i', 'n', 'g'], ['a', 'n', 'd'], ['i'], ['t', 'h', 'i', 'n', 'k'], ['i', 't'], ['i', 's'], ['e', 's', 's', 'e', 'n', 't', 'i', 'a', 'l'], ['t', 'h', 'a', 't'], ['w', 'e'], ['s', 'e', 'e'], ['t', 'h', 'e'], ['e', 'u', 'r', 'o'], ['a', 's'], ['a'], ['k', 'e', 'y'], ['p', 'o', 'l', 'i', 'c', 'y'], ['i', 'n', 's', 't', 'r', 'u', 'm', 'e', 'n', 't'], ['f', 'o', 'r'], ['e', 'c', 'o', 'n', 'o', 'm', 'i', 'c'], ['p', 'o', 'l', 'i', 'c', 'y'], ['a', 'n', 'd'], ['s', 'u', 's', 't', 'a', 'i', 'n', 'a', 'b', 'l', 'e'], ['g', 'r', 'o', 'w', 't', 'h'], ['i', 'n'], ['e', 'u', 'r', 'o', 'p', 'e'], ['a', 'n', 'd'], ['a', 't'], ['t', 'h', 'e'], ['s', 'a', 'm', 'e'], ['t', 'i', 'm', 'e'], ['t', 'h', 'a', 't'], ['w', 'e'], ['l', 'o', 'o', 'k'], ['a', 't'], ['i', 't'], ['n', 'o', 't'], ['o', 'n', 'l', 'y'], ['a', 's'], ['a'], ['s', 'y', 'm', 'b', 'o', 'l'], ['b', 'u', 't'], ['a', 'l', 's', 'o'], ['a', 's'], ['a'], ['b', 'o', 'n', 'd'], ['f', 'o', 'r'], ['e', 'u', 'r', 'o', 'p', 'e', 'a', 'n', 's'], ['i', 'n'], ['b', 'u', 'i', 'l', 'd', 'i', 'n', 'g'], ['t', 'h', 'e'], ['c', 'o', 'm', 'm', 'o', 'n'], ['e', 'u', 'r', 'o', 'p', 'e', 'a', 'n'], ['h', 'o', 'm', 'e'], ['i', 'n'], ['t', 'h', 'i', 's'], ['r', 'e', 'g', 'a', 'r', 'd'], ['w', 'e'], ['h', 'a', 'v', 'e'], ['o', 'r', 'g', 'a', 'n', 'i', 's', 'e', 'd'], ['a'], ['b', 'r', 'o', 'a', 'd'], ['r', 'a', 'n', 'g', 'e'], ['o', 'f'], ['c', 'o', 'm', 'm', 'u', 'n', 'i', 'c', 'a', 't', 'i', 'o', 'n'], ['a', 'c', 't', 'i', 'v', 'i', 't', 'i', 'e', 's'], ['f', 'o', 'r'], ['i', 'n', 's', 't', 'a', 'n', 'c', 'e'], ['i', 'n'], ['t', 'h', 'e'], ['f', 'i', 'r', 's', 't'], ['h', 'a', 'l', 'f'], ['o', 'f'], ['i'], ['w', 'i', 'l', 'l'], ['j', 'u', 's', 't'], ['g', 'i', 'v', 'e'], ['y', 'o', 'u'], ['t', 'h', 'e'], ['h', 'i', 'g', 'h', 'l', 'i', 'g', 'h', 't', 's'], ['w', 'e'], ['c', 'a', 'r', 'r', 'i', 'e', 'd'], ['o', 'u', 't'], ['a', 'n'], ['i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n'], ['c', 'a', 'm', 'p', 'a', 'i', 'g', 'n'], ['o', 'n'], ['t', 'h', 'e'], ['k', 'e', 'y'], ['b', 'e', 'n', 'e', 'f', 'i', 't', 's'], ['o', 'f'], ['t', 'h', 'e'], ['e', 'u', 'r', 'o'], ['i', 'n'], ['e', 'u', 'r', 'o'], ['a', 'r', 'e', 'a'], ['c', 'o', 'u', 'n', 't', 'r', 'i', 'e', 's'], ['g', 'e', 'r', 'm', 'a', 'n', 'y'], ['f', 'r', 'a', 'n', 'c', 'e'], ['i', 't', 'a', 'l', 'y'], ['p', 'o', 'r', 't', 'u', 'g', 'a', 'l'], ['t', 'h', 'e'], ['n', 'e', 't', 'h', 'e', 'r', 'l', 'a', 'n', 'd', 's'], ['s', 'p', 'a', 'i', 'n'], ['a', 'u', 's', 't', 'r', 'i', 'a'], ['f', 'i', 'n', 'l', 'a', 'n', 'd'], ['m', 'a', 'l', 't', 'a'], ['a', 'n', 'd'], ['b', 'e', 'l', 'g', 'i', 'u', 'm'], ['w', 'e'], ['a', 'l', 's', 'o'], ['o', 'r', 'g', 'a', 'n', 'i', 's', 'e', 'd'], ['t', 'h', 'e'], ['b', 'r', 'u', 's', 's', 'e', 'l', 's'], ['e', 'c', 'o', 'n', 'o', 'm', 'i', 'c'], ['f', 'o', 'r', 'u', 'm'], ['d', 'e', 'v', 'o', 't', 'e', 'd'], ['t', 'o'], ['e', 'c', 'o', 'n', 'o', 'm', 'i', 'c'], ['a', 'n', 'd'], ['f', 'i', 'n', 'a', 'n', 'c', 'i', 'a', 'l'], ['i', 's', 's', 'u', 'e', 's'], ['w', 'h', 'i', 'c', 'h'], ['t', 'o', 'o', 'k'], ['p', 'l', 'a', 'c', 'e'], ['t', 'w', 'o'], ['w', 'e', 'e', 'k', 's'], ['a', 'g', 'o'], ['a'], ['r', 'e', 'c', 'o', 'r', 'd'], ['n', 'u', 'm', 'b', 'e', 'r'], ['o', 'f'], ['p', 'o', 'l', 'i', 'c', 'y'], ['a', 'r', 'e', 'a', 's'], ['m', 'o', 'r', 'e'], ['t', 'h', 'a', 'n'], ['w', 'e', 'r', 'e'], ['a', 'g', 'r', 'e', 'e', 'd'], ['o', 'n'], ['a', 'n', 'd'], ['t', 'h', 'e'], ['e', 'v', 'e', 'n', 't'], ['w', 'a', 's'], ['w', 'i', 'd', 'e', 'l', 'y'], ['r', 'e', 'p', 'o', 'r', 't', 'e', 'd'], ['b', 'y'], ['t', 'h', 'e'], ['m', 'e', 'd', 'i', 'a'], ['t', 'h', 'i', 's'], ['w', 'a', 's'], ['c', 'l', 'e', 'a', 'r', 'l', 'y'], ['a'], ['c', 'a', 's', 'e'], ['o', 'f'], ['r', 'e', 'a', 'c', 'h', 'i', 'n', 'g'], ['o', 'u', 't'], ['t', 'o'], ['t', 'h', 'e'], ['o', 'p', 'i', 'n', 'i', 'o', 'n'], ['m', 'a', 'k', 'e', 'r', 's'], ['a', 'n', 'd'], ['t', 'r', 'y', 'i', 'n', 'g'], ['t', 'o'], ['h', 'a', 'v', 'e'], ['a'], ['m', 'u', 'l', 't', 'i', 'p', 'l', 'i', 'e', 'r'], ['e', 'f', 'f', 'e', 'c', 't'], ['t', 'h', 'r', 'o', 'u', 'g', 'h'], ['t', 'h', 'i', 's'], ['i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't'], ['f', 'o', 'r', 'u', 'm'], ['i'], ['c', 'o', 'u', 'l', 'd'], ['c', 'o', 'n', 't', 'i', 'n', 'u', 'e'], ['t', 'h', 'e'], ['l', 'i', 's', 't'], ['w', 'e'], ['a', 'r', 'e'], ['e', 'n', 'g', 'a', 'g', 'e', 'd'], ['i', 'n'], ['t', 'h', 'e', 's', 'e'], ['a', 'c', 't', 'i', 'v', 'i', 't', 'i', 'e', 's'], ['a', 'n', 'd'], ['i'], ['t', 'h', 'i', 'n', 'k'], ['w', 'e'], ['w', 'i', 'l', 'l'], ['a', 'c', 'h', 'i', 'e', 'v', 'e'], ['t', 'h', 'e'], ['b', 'e', 's', 't'], ['r', 'e', 's', 'u', 'l', 't', 's'], ['b', 'y'], ['h', 'a', 'v', 'i', 'n', 'g'], ['t', 'h', 'e'], ['r', 'i', 'g', 'h', 't'], ['p', 'o', 'l', 'i', 'c', 'i', 'e', 's'], ['a', 'n', 'd'], ['h', 'a', 'v', 'i', 'n', 'g'], ['t', 'h', 'e'], ['r', 'i', 'g', 'h', 't'], ['k', 'i', 'n', 'd'], ['o', 'f'], ['c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', 's'], ['i', 'n'], ['o', 'r', 'd', 'e', 'r'], ['t', 'o'], ['e', 'n', 's', 'u', 'r', 'e'], ['t', 'h', 'a', 't'], ['o', 'u', 'r'], ['p', 'e', 'o', 'p', 'l', 'e'], ['a', 'r', 'e'], ['w', 'e', 'l', 'l'], ['i', 'n', 'f', 'o', 'r', 'm', 'e', 'd'], ['i', 'n'], ['m', 'y'], ['y', 'o', 'u', 't', 'h'], ['i'], ['t', 'r', 'a', 'i', 'n', 'e', 'd'], ['a', 's'], ['a'], ['s', 'p', 'a', 'r', 'e'], ['p', 'a', 'r', 't', 's'], ['s', 'a', 'l', 'e', 's', 'm', 'a', 'n'], ['i'], ['s', 't', 'a', 'r', 't', 'e', 'd'], ['a', 't'], ['a', 'n', 'd'], ['i'], ['e', 'n', 'd', 'e', 'd'], ['t', 'h', 'a', 't'], ['c', 'a', 'r', 'e', 'e', 'r'], ['a', 't'], ['w', 'e'], ['a', 'l', 'w', 'a', 'y', 's'], ['h', 'a', 'd'], ['a'], ['v', 'e', 'r', 'y'], ['c', 'l', 'e', 'a', 'r'], ['p', 'h', 'i', 'l', 'o', 's', 'o', 'p', 'h', 'y'], ['t', 'h', 'a', 't'], ['y', 'o', 'u'], ['h', 'a', 'v', 'e'], ['t', 'o'], ['h', 'a', 'v', 'e'], ['t', 'h', 'e'], ['p', 'r', 'o', 'd', 'u', 'c', 't'], ['r', 'i', 'g', 'h', 't'], ['b', 'e', 'f', 'o', 'r', 'e'], ['y', 'o', 'u'], ['c', 'a', 'n'], ['m', 'a', 'r', 'k', 'e', 't'], ['i', 't'], ['w', 'e', 'l', 'l'], ['y', 'o', 'u'], ['n', 'e', 'e', 'd'], ['b', 'o', 't', 'h'], ['y', 'o', 'u'], ['n', 'e', 'e', 'd'], ['a'], ['s', 'o', 'l', 'i', 'd'], ['p', 'r', 'o', 'd', 'u', 'c', 't'], ['s', 'o', 'l', 'i', 'd'], ['p', 'o', 'l', 'i', 'c', 'i', 'e', 's'], ['a', 'n', 'd'], ['y', 'o', 'u'], ['n', 'e', 'e', 'd'], ['a'], ['s', 'o', 'u', 'n', 'd'], ['m', 'a', 'r', 'k', 'e', 't', 'i', 'n', 'g'], ['c', 'a', 'm', 'p', 'a', 'i', 'g', 'n'], ['i', 'n'], ['o', 'r', 'd', 'e', 'r'], ['t', 'o'], ['w', 'i', 'n'], ['t', 'h', 'e'], ['h', 'e', 'a', 'r', 't', 's'], ['a', 'n', 'd'], ['m', 'i', 'n', 'd', 's'], ['o', 'f'], ['o', 'u', 'r'], ['p', 'e', 'o', 'p', 'l', 'e']]\n",
      "Training corpus size: 6117\n",
      "Example preprocessed sentence (train corpus): ['m', 'e', 'm', 'b', 'e', 'r', 'o', 'f', 't', 'h', 'e', 'c', 'o', 'm', 'm', 'i', 's', 's', 'i', 'o', 'n', 'i', 'a', 'g', 'r', 'e', 'e', 'w', 'i', 't', 'h', 'y', 'o', 'u', 'r', 'w', 'a', 'y', 'o', 'f', 'r', 'e', 'a', 's', 'o', 'n', 'i', 'n', 'g', 'a', 'n', 'd', 'i', 't', 'h', 'i', 'n', 'k', 'i', 't', 'i', 's', 'e', 's', 's', 'e', 'n', 't', 'i', 'a', 'l', 't', 'h', 'a', 't', 'w', 'e', 's', 'e', 'e', 't', 'h', 'e', 'e', 'u', 'r', 'o', 'a', 's', 'a', 'k', 'e', 'y', 'p', 'o', 'l', 'i', 'c', 'y', 'i', 'n', 's', 't', 'r', 'u', 'm', 'e', 'n', 't', 'f', 'o', 'r', 'e', 'c', 'o', 'n', 'o', 'm', 'i', 'c', 'p', 'o', 'l', 'i', 'c', 'y', 'a', 'n', 'd', 's', 'u', 's', 't', 'a', 'i', 'n', 'a', 'b', 'l', 'e', 'g', 'r', 'o', 'w', 't', 'h', 'i', 'n', 'e', 'u', 'r', 'o', 'p', 'e', 'a', 'n', 'd', 'a', 't', 't', 'h', 'e', 's', 'a', 'm', 'e', 't', 'i', 'm', 'e', 't', 'h', 'a', 't', 'w', 'e', 'l', 'o', 'o', 'k', 'a', 't', 'i', 't', 'n', 'o', 't', 'o', 'n', 'l', 'y', 'a', 's', 'a', 's', 'y', 'm', 'b', 'o', 'l', 'b', 'u', 't', 'a', 'l', 's', 'o', 'a', 's', 'a', 'b', 'o', 'n', 'd', 'f', 'o', 'r', 'e', 'u', 'r', 'o', 'p', 'e', 'a', 'n', 's', 'i', 'n', 'b', 'u', 'i', 'l', 'd', 'i', 'n', 'g', 't', 'h', 'e', 'c', 'o', 'm', 'm', 'o', 'n', 'e', 'u', 'r', 'o', 'p', 'e', 'a', 'n', 'h', 'o', 'm', 'e', 'i', 'n', 't', 'h', 'i', 's', 'r', 'e', 'g', 'a', 'r', 'd', 'w', 'e', 'h', 'a', 'v', 'e', 'o', 'r', 'g', 'a', 'n', 'i', 's', 'e', 'd', 'a', 'b', 'r', 'o', 'a', 'd', 'r', 'a', 'n', 'g', 'e', 'o', 'f', 'c', 'o', 'm', 'm', 'u', 'n', 'i', 'c', 'a', 't', 'i', 'o', 'n', 'a', 'c', 't', 'i', 'v', 'i', 't', 'i', 'e', 's', 'f', 'o', 'r', 'i', 'n', 's', 't', 'a', 'n', 'c', 'e', 'i', 'n', 't', 'h', 'e', 'f', 'i', 'r', 's', 't', 'h', 'a', 'l', 'f', 'o', 'f', 'i', 'w', 'i', 'l', 'l', 'j', 'u', 's', 't', 'g', 'i', 'v', 'e', 'y', 'o', 'u', 't', 'h', 'e', 'h', 'i', 'g', 'h', 'l', 'i', 'g', 'h', 't', 's', 'w', 'e', 'c', 'a', 'r', 'r', 'i', 'e', 'd', 'o', 'u', 't', 'a', 'n', 'i', 'n', 'f', 'o', 'r', 'm', 'a', 't', 'i', 'o', 'n', 'c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', 'o', 'n', 't', 'h', 'e', 'k', 'e', 'y', 'b', 'e', 'n', 'e', 'f', 'i', 't', 's', 'o', 'f', 't', 'h', 'e', 'e', 'u', 'r', 'o', 'i', 'n', 'e', 'u', 'r', 'o', 'a', 'r', 'e', 'a', 'c', 'o', 'u', 'n', 't', 'r', 'i', 'e', 's', 'g', 'e', 'r', 'm', 'a', 'n', 'y', 'f', 'r', 'a', 'n', 'c', 'e', 'i', 't', 'a', 'l', 'y', 'p', 'o', 'r', 't', 'u', 'g', 'a', 'l', 't', 'h', 'e', 'n', 'e', 't', 'h', 'e', 'r', 'l', 'a', 'n', 'd', 's', 's', 'p', 'a', 'i', 'n', 'a', 'u', 's', 't', 'r', 'i', 'a', 'f', 'i', 'n', 'l', 'a', 'n', 'd', 'm', 'a', 'l', 't', 'a', 'a', 'n', 'd', 'b', 'e', 'l', 'g', 'i', 'u', 'm', 'w', 'e', 'a', 'l', 's', 'o', 'o', 'r', 'g', 'a', 'n', 'i', 's', 'e', 'd', 't', 'h', 'e', 'b', 'r', 'u', 's', 's', 'e', 'l', 's', 'e', 'c', 'o', 'n', 'o', 'm', 'i', 'c', 'f', 'o', 'r', 'u', 'm', 'd', 'e', 'v', 'o', 't', 'e', 'd', 't', 'o', 'e', 'c', 'o', 'n', 'o', 'm', 'i', 'c', 'a', 'n', 'd', 'f', 'i', 'n', 'a', 'n', 'c', 'i', 'a', 'l', 'i', 's', 's', 'u', 'e', 's', 'w', 'h', 'i', 'c', 'h', 't', 'o', 'o', 'k', 'p', 'l', 'a', 'c', 'e', 't', 'w', 'o', 'w', 'e', 'e', 'k', 's', 'a', 'g', 'o', 'a', 'r', 'e', 'c', 'o', 'r', 'd', 'n', 'u', 'm', 'b', 'e', 'r', 'o', 'f', 'p', 'o', 'l', 'i', 'c', 'y', 'a', 'r', 'e', 'a', 's', 'm', 'o', 'r', 'e', 't', 'h', 'a', 'n', 'w', 'e', 'r', 'e', 'a', 'g', 'r', 'e', 'e', 'd', 'o', 'n', 'a', 'n', 'd', 't', 'h', 'e', 'e', 'v', 'e', 'n', 't', 'w', 'a', 's', 'w', 'i', 'd', 'e', 'l', 'y', 'r', 'e', 'p', 'o', 'r', 't', 'e', 'd', 'b', 'y', 't', 'h', 'e', 'm', 'e', 'd', 'i', 'a', 't', 'h', 'i', 's', 'w', 'a', 's', 'c', 'l', 'e', 'a', 'r', 'l', 'y', 'a', 'c', 'a', 's', 'e', 'o', 'f', 'r', 'e', 'a', 'c', 'h', 'i', 'n', 'g', 'o', 'u', 't', 't', 'o', 't', 'h', 'e', 'o', 'p', 'i', 'n', 'i', 'o', 'n', 'm', 'a', 'k', 'e', 'r', 's', 'a', 'n', 'd', 't', 'r', 'y', 'i', 'n', 'g', 't', 'o', 'h', 'a', 'v', 'e', 'a', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'i', 'e', 'r', 'e', 'f', 'f', 'e', 'c', 't', 't', 'h', 'r', 'o', 'u', 'g', 'h', 't', 'h', 'i', 's', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't', 'f', 'o', 'r', 'u', 'm', 'i', 'c', 'o', 'u', 'l', 'd', 'c', 'o', 'n', 't', 'i', 'n', 'u', 'e', 't', 'h', 'e', 'l', 'i', 's', 't', 'w', 'e', 'a', 'r', 'e', 'e', 'n', 'g', 'a', 'g', 'e', 'd', 'i', 'n', 't', 'h', 'e', 's', 'e', 'a', 'c', 't', 'i', 'v', 'i', 't', 'i', 'e', 's', 'a', 'n', 'd', 'i', 't', 'h', 'i', 'n', 'k', 'w', 'e', 'w', 'i', 'l', 'l', 'a', 'c', 'h', 'i', 'e', 'v', 'e', 't', 'h', 'e', 'b', 'e', 's', 't', 'r', 'e', 's', 'u', 'l', 't', 's', 'b', 'y', 'h', 'a', 'v', 'i', 'n', 'g', 't', 'h', 'e', 'r', 'i', 'g', 'h', 't', 'p', 'o', 'l', 'i', 'c', 'i', 'e', 's', 'a', 'n', 'd', 'h', 'a', 'v', 'i', 'n', 'g', 't', 'h', 'e', 'r', 'i', 'g', 'h', 't', 'k', 'i', 'n', 'd', 'o', 'f', 'c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', 's', 'i', 'n', 'o', 'r', 'd', 'e', 'r', 't', 'o', 'e', 'n', 's', 'u', 'r', 'e', 't', 'h', 'a', 't', 'o', 'u', 'r', 'p', 'e', 'o', 'p', 'l', 'e', 'a', 'r', 'e', 'w', 'e', 'l', 'l', 'i', 'n', 'f', 'o', 'r', 'm', 'e', 'd', 'i', 'n', 'm', 'y', 'y', 'o', 'u', 't', 'h', 'i', 't', 'r', 'a', 'i', 'n', 'e', 'd', 'a', 's', 'a', 's', 'p', 'a', 'r', 'e', 'p', 'a', 'r', 't', 's', 's', 'a', 'l', 'e', 's', 'm', 'a', 'n', 'i', 's', 't', 'a', 'r', 't', 'e', 'd', 'a', 't', 'a', 'n', 'd', 'i', 'e', 'n', 'd', 'e', 'd', 't', 'h', 'a', 't', 'c', 'a', 'r', 'e', 'e', 'r', 'a', 't', 'w', 'e', 'a', 'l', 'w', 'a', 'y', 's', 'h', 'a', 'd', 'a', 'v', 'e', 'r', 'y', 'c', 'l', 'e', 'a', 'r', 'p', 'h', 'i', 'l', 'o', 's', 'o', 'p', 'h', 'y', 't', 'h', 'a', 't', 'y', 'o', 'u', 'h', 'a', 'v', 'e', 't', 'o', 'h', 'a', 'v', 'e', 't', 'h', 'e', 'p', 'r', 'o', 'd', 'u', 'c', 't', 'r', 'i', 'g', 'h', 't', 'b', 'e', 'f', 'o', 'r', 'e', 'y', 'o', 'u', 'c', 'a', 'n', 'm', 'a', 'r', 'k', 'e', 't', 'i', 't', 'w', 'e', 'l', 'l', 'y', 'o', 'u', 'n', 'e', 'e', 'd', 'b', 'o', 't', 'h', 'y', 'o', 'u', 'n', 'e', 'e', 'd', 'a', 's', 'o', 'l', 'i', 'd', 'p', 'r', 'o', 'd', 'u', 'c', 't', 's', 'o', 'l', 'i', 'd', 'p', 'o', 'l', 'i', 'c', 'i', 'e', 's', 'a', 'n', 'd', 'y', 'o', 'u', 'n', 'e', 'e', 'd', 'a', 's', 'o', 'u', 'n', 'd', 'm', 'a', 'r', 'k', 'e', 't', 'i', 'n', 'g', 'c', 'a', 'm', 'p', 'a', 'i', 'g', 'n', 'i', 'n', 'o', 'r', 'd', 'e', 'r', 't', 'o', 'w', 'i', 'n', 't', 'h', 'e', 'h', 'e', 'a', 'r', 't', 's', 'a', 'n', 'd', 'm', 'i', 'n', 'd', 's', 'o', 'f', 'o', 'u', 'r', 'p', 'e', 'o', 'p', 'l', 'e']\n"
     ]
    }
   ],
   "source": [
    "# English sentences in the training set, after getting rid of all white spaces\n",
    "english_training_sentences = [''.join(train_corpus[sentence_id]['en']) for sentence_id in train_corpus]\n",
    "english_training_sentences_no_spaces = [list(sentence.replace(' ', '')) for sentence in english_training_sentences]\n",
    "print(english_training_sentences_no_spaces[0])\n",
    "\n",
    "all_chars = ''.join([''.join(sentence) for sentence in english_training_sentences_no_spaces])\n",
    "char_freq = Counter(all_chars)\n",
    "frequent_chars = {char for char, freq in char_freq.items() if freq >= 20}\n",
    "english_words = set(word for sentence in english_training_sentences for word in sentence.split())\n",
    "\n",
    "english_word_types_prepared = [\n",
    "    ''.join([char if char in frequent_chars else '?' for char in word])\n",
    "    for word in english_words\n",
    "]\n",
    "\n",
    "english_word_types_sentences = [list(word) for word in english_word_types_prepared]\n",
    "sentence_as_lists_of_chars_per_word = [\n",
    "    [list(word) for word in sentence.split()] for sentence in english_training_sentences\n",
    "]\n",
    "print(sentence_as_lists_of_chars_per_word[0])\n",
    "\n",
    "# Train the LMs\n",
    "corpus_sent = Corpus(english_training_sentences_no_spaces, t=20, n=2, bos_eos=True).sentences\n",
    "lm_sent_2gram = LM(corpus_sent, n=2, k=0.01)\n",
    "\n",
    "corpus_word_types = english_word_types_sentences\n",
    "lm_word_2gram = LM(corpus_word_types, n=2, k=0.01)\n",
    "\n",
    "corpus_sent = Corpus(english_training_sentences_no_spaces, t=20, n=4, bos_eos=True).sentences\n",
    "lm_sent_4gram = LM(corpus_sent, n=4, k=0.01)\n",
    "\n",
    "corpus_word_types = english_word_types_sentences\n",
    "lm_word_4gram = LM(corpus_word_types, n=4, k=0.01)\n",
    "\n",
    "# Save the LMs\n",
    "lm_sent_2gram.save('BelizPekkan_sents_2gr_en.pkl')\n",
    "lm_word_2gram.save('BelizPekkan_words_2gr_en.pkl')\n",
    "lm_sent_4gram.save('BelizPekkan_sents_4gr_en.pkl')\n",
    "lm_word_4gram.save('BelizPekkan_words_4gr_en.pkl')\n",
    "\n",
    "# Output the size of the corpora and example preprocessed sentence\n",
    "print(f\"Training corpus size: {len(train_corpus)}\")\n",
    "print(f\"Example preprocessed sentence (train corpus): {english_training_sentences_no_spaces[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b126e",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22156a3",
   "metadata": {},
   "source": [
    "3. Compute the perplexity of all four Language Models on:\n",
    "\t- the English sentences from the training set\n",
    "\t- the English word types from the training set\n",
    "\t- the English sentences from the test set\n",
    "\t- the Dutch sentences from the test set\n",
    "\t- the Italian sentences from the test set\n",
    "\n",
    "You should submit a .csv file with the following structure, column names, and values ([2/4] means either 2 for LMs predicting based on two previous characters or 4 for LMs predicting based on four previous characters, the options under test_data indicate the five sets to be used to compute perplexity):\n",
    "\n",
    "|ngram_size|training_data|test_data|perplexity|\n",
    "|---|---|---|---|\n",
    "|[2/4]|[words/sents]|[ITtest/NLtest/ENtest/ENtrain\\_sents/ENtrain\\_words]|float (rounded at 4 decimal places)|\n",
    "|---|---|---|---|\n",
    "\n",
    "The file should be named according to the template Name(Initial)Surname\\_perplexities.csv\n",
    "\n",
    "> 5 points available: you get 1 point if all four LMs yield the correct perplexity scores for a test_dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a9ecc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity results saved to BelizPekkan_perplexities.csv\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexities(lm, test_sentences, ngram_size, training_data, test_data_label):\n",
    "    perplexity = lm.perplexity(test_sentences)\n",
    "    return {\n",
    "        \"ngram_size\": ngram_size,\n",
    "        \"training_data\": training_data,\n",
    "        \"test_data\": test_data_label,\n",
    "        \"perplexity\": round(perplexity, 4)\n",
    "    }\n",
    "\n",
    "# List to store perplexity results\n",
    "perplexity_results = []\n",
    "\n",
    "# Compute perplexities for each combination\n",
    "perplexity_results.append(compute_perplexities(lm_sent_2gram, english_training_sentences_no_spaces, 2, \"sents\", \"ENtrain_sents\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_2gram, english_training_sentences_no_spaces, 2, \"words\", \"ENtrain_sents\"))\n",
    "perplexity_results.append(compute_perplexities(lm_sent_4gram, english_training_sentences_no_spaces, 4, \"sents\", \"ENtrain_sents\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_4gram, english_training_sentences_no_spaces, 4, \"words\", \"ENtrain_sents\"))\n",
    "\n",
    "perplexity_results.append(compute_perplexities(lm_sent_2gram, english_word_types_sentences, 2, \"sents\", \"ENtrain_words\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_2gram, english_word_types_sentences, 2, \"words\", \"ENtrain_words\"))\n",
    "perplexity_results.append(compute_perplexities(lm_sent_4gram, english_word_types_sentences, 4, \"sents\", \"ENtrain_words\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_4gram, english_word_types_sentences, 4, \"words\", \"ENtrain_words\"))\n",
    "\n",
    "perplexity_results.append(compute_perplexities(lm_sent_2gram, english_test_sentences_no_spaces, 2, \"sents\", \"ENtest\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_2gram, english_test_sentences_no_spaces, 2, \"words\", \"ENtest\"))\n",
    "perplexity_results.append(compute_perplexities(lm_sent_4gram, english_test_sentences_no_spaces, 4, \"sents\", \"ENtest\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_4gram, english_test_sentences_no_spaces, 4, \"words\", \"ENtest\"))\n",
    "\n",
    "perplexity_results.append(compute_perplexities(lm_sent_2gram, dutch_test_sentences_no_spaces, 2, \"sents\", \"NLtest\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_2gram, dutch_test_sentences_no_spaces, 2, \"words\", \"NLtest\"))\n",
    "perplexity_results.append(compute_perplexities(lm_sent_4gram, dutch_test_sentences_no_spaces, 4, \"sents\", \"NLtest\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_4gram, dutch_test_sentences_no_spaces, 4, \"words\", \"NLtest\"))\n",
    "\n",
    "perplexity_results.append(compute_perplexities(lm_sent_2gram, italian_test_sentences_no_spaces, 2, \"sents\", \"ITtest\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_2gram, italian_test_sentences_no_spaces, 2, \"words\", \"ITtest\"))\n",
    "perplexity_results.append(compute_perplexities(lm_sent_4gram, italian_test_sentences_no_spaces, 4, \"sents\", \"ITtest\"))\n",
    "perplexity_results.append(compute_perplexities(lm_word_4gram, italian_test_sentences_no_spaces, 4, \"words\", \"ITtest\"))\n",
    "\n",
    "# Define the output CSV file name\n",
    "csv_file_name = \"BelizPekkan_perplexities.csv\"\n",
    "\n",
    "# Write the results to the CSV file\n",
    "with open(csv_file_name, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"ngram_size\", \"training_data\", \"test_data\", \"perplexity\"])\n",
    "    writer.writeheader()\n",
    "    for result in perplexity_results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(f\"Perplexity results saved to {csv_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024d5b2",
   "metadata": {},
   "source": [
    "## Task4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4654809c",
   "metadata": {},
   "source": [
    "4. Out of all Italian and Dutch word types in the test sentences, restricting attention to word types consisting of at least 5 characters and with at least 5 occurrences in the Italian/Dutch test sentences, find:\n",
    "\t- the word in each language with the lowest perplexity according to each of the four LMs\n",
    "\t- the word in each language with the highest perplexity according to each of four LMs\n",
    "\n",
    "You should submit two .csv files (one for the lowest perplexities, one for highest perplexities) with the following structure, column names, and values ([it/nl] indicates the language, with it indicating italian and nl indicating dutch, str indicates that the word should appear as a string, [2/4] means either 2 for LMs predicting based on two previous characters or 4 for LMs predicting based on four previous characters, [words/sents] indicates whether the model identifying that particular word on that language was trained on word types or sentences):\n",
    "\n",
    "| lang | word | ngram_size | training_data | perplexity |\n",
    "|---|---|---|---|---|\n",
    "|[it/nl]|str|[2/4]|[words/sents]|float (rounded at 4 decimal places)|\n",
    "|---|---|---|---|\n",
    "\n",
    "The files should be named according to the template Name(Initial)Surname\\_perplexities\\_[max|min].csv, so Jane Smith should submit a file named JaneSmith\\_perplexities\\_max.csv containing 8 rows each storing the word with the highest perplexity according to each of the four LMs per language.\n",
    "\n",
    "> 4 points available: you get 0.25 points for each correct word identified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d762122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest perplexity results saved to BelizPekkan_perplexities_min.csv\n",
      "Highest perplexity results saved to BelizPekkan_perplexities_max.csv\n"
     ]
    }
   ],
   "source": [
    "def preprocess_words(word, frequent_chars):\n",
    "    return ''.join([char if char in frequent_chars else '?' for char in word])\n",
    "\n",
    "def extract_frequent_words(sentences, min_length=5, min_occurrences=5):\n",
    "    words = [''.join(word) for sentence in sentences for word in sentence.split()]\n",
    "    filtered_words = [word for word in words if len(word) >= min_length]\n",
    "    word_counts = Counter(filtered_words)\n",
    "    return [word for word, count in word_counts.items() if count >= min_occurrences]\n",
    "\n",
    "# Extract word types from Italian and Dutch test sentences\n",
    "italian_words = extract_frequent_words(italian_test_sentences, min_length=5, min_occurrences=5)\n",
    "dutch_words = extract_frequent_words(dutch_test_sentences, min_length=5, min_occurrences=5)\n",
    "\n",
    "def compute_word_perplexity(model, word):\n",
    "    word_processed = preprocess_words(word, model.vocab)\n",
    "    word_ngrams = ['#bos#'] * (model.ngram_size - 1) + list(word_processed) + ['#eos#']\n",
    "    log_prob = 0.0\n",
    "    for i in range(len(word_ngrams) - model.ngram_size + 1):\n",
    "        ngram = tuple(word_ngrams[i:i + model.ngram_size])\n",
    "        history = ngram[:-1] if model.ngram_size > 1 else ()\n",
    "        char = ngram[-1]\n",
    "        prob = model.calculate_probability(history, char)\n",
    "        log_prob += np.log(prob if prob > 0 else 1e-10)\n",
    "    perplexity = np.exp(-log_prob / len(word_ngrams))\n",
    "    return perplexity\n",
    "\n",
    "def filter_eligible_words(sentences):\n",
    "    word_counts = Counter(word for sentence in sentences for word in sentence.split())\n",
    "    return {word for word, count in word_counts.items() if len(word) > 4 and count >= 5}\n",
    "\n",
    "# Initialize models dictionary\n",
    "models = {\n",
    "    'sent_2gram': lm_sent_2gram,\n",
    "    'word_2gram': lm_word_2gram,\n",
    "    'sent_4gram': lm_sent_4gram,\n",
    "    'word_4gram': lm_word_4gram,\n",
    "}\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "for lang, eligible_words in [(\"it\", italian_words), (\"nl\", dutch_words)]:\n",
    "    for word in eligible_words:\n",
    "        for model_key, model in models.items():\n",
    "            parts = model_key.split(\"_\")\n",
    "            ngram_size = \"2\" if \"2gram\" in model_key else \"4\"\n",
    "            training_data_type = \"words\" if \"word\" in model_key else \"sents\"\n",
    "            perplexity = compute_word_perplexity(model, word)\n",
    "            results.append([lang, word, ngram_size, training_data_type, round(perplexity, 4)])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['lang', 'word', 'ngram_size', 'training_data', 'perplexity'])\n",
    "\n",
    "# Find min and max perplexities\n",
    "min_perplexities_df = results_df.loc[results_df.groupby(['lang', 'ngram_size', 'training_data'])['perplexity'].idxmin()]\n",
    "max_perplexities_df = results_df.loc[results_df.groupby(['lang', 'ngram_size', 'training_data'])['perplexity'].idxmax()]\n",
    "\n",
    "# Define the output CSV file names\n",
    "min_csv_file_name = \"BelizPekkan_perplexities_min.csv\"\n",
    "max_csv_file_name = \"BelizPekkan_perplexities_max.csv\"\n",
    "\n",
    "# Write the lowest perplexities to the CSV file\n",
    "min_perplexities_df.to_csv(min_csv_file_name, index=False)\n",
    "\n",
    "# Write the highest perplexities to the CSV file\n",
    "max_perplexities_df.to_csv(max_csv_file_name, index=False)\n",
    "\n",
    "print(f\"Lowest perplexity results saved to {min_csv_file_name}\")\n",
    "print(f\"Highest perplexity results saved to {max_csv_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513369b",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "Answer questions in the separate markdown blocks below.\n",
    "\n",
    "5. Answer the following questions:\n",
    "\t- a. compare LMs' perplexity on the English training sets, sentences and words, then explain the differences in perplexity considering what changes between the two training set-ups. (5 pts, 150 words)\n",
    "\t- b. which LM trained on sentences generalizes better to unseen sentences in the same language, bigram or tetragram? explain why this is the case. (5 pts, 150 words)\n",
    "\t- c. compare LMs trained on English in their ability to fit Italian and Dutch sentences: which factor between ngram size and training corpus (words or sentences) affects perplexity the most? Explain why we observe this pattern. (4 pts, 100 words)\n",
    "\t- d. what patterns can you identify in the words with the lowest perplexity in Dutch and Italian? (4 pts, 100 words)\n",
    "\t- e. what patterns can you identify in the words with the highest perplexity in Dutch and Italian? (4 pts, 100 words)\n",
    "    \n",
    "> 22 points in total, see specifications next to each question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea21e8",
   "metadata": {},
   "source": [
    "#### 5a\n",
    "\n",
    "2-gram models trained on sentences have lower perplexity (11.8622) than those trained on word types (15.7788), due to the broader context sentences provide. 4-gram models trained on sentences have even lower perplexity (5.3673), benefiting from longer sequences. However, 4-gram models trained on word types have higher perplexity (21.1391), as the advantage of longer n-grams is less effective for isolated word types. For English training word types, 2-gram models trained on word types have lower perplexity (12.4359) than those trained on sentences (28.4246), suggesting word-based models are more effective for shorter n-grams. However, 4-gram models trained on word types have lower perplexity (6.1067) than 2-gram models, indicating longer n-grams benefit from additional context. Overall, sentence-trained models perform better with longer n-grams, while word type models perform better with shorter n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1547f27",
   "metadata": {},
   "source": [
    "#### 5b\n",
    "\n",
    "The tetragram LM trained on sentences has a perplexity of 5.3673, which is significantly lower than the perplexity of the bigram LM trained on sentences, which is 11.8622. A lower perplexity indicates that the model is better at predicting the next character in a sequence, suggesting it generalizes better to unseen data. This is the case because tetragram models capture more context and dependencies in the data. While a bigram model only considers the immediate previous character, a tetragram model takes into account the previous three characters, providing a richer context for making predictions. This additional context allows the tetragram model to better understand the structure and patterns within the language, leading to more accurate predictions and lower perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b34c68",
   "metadata": {},
   "source": [
    "#### 5c\n",
    "\n",
    "N-gram size has a more significant effect on perplexity than the training corpus when comparing how well LMs trained on English fit Italian and Dutch sentences. This is evident from the consistently lower perplexity values for 2-gram models compared to 4-gram models, irrespective of whether they were trained on words or sentences. The reason for this pattern is that shorter n-gram models (like 2-gram models) are less sensitive to the specific sequences within a language, making them more robust to cross-linguistic variations. Longer n-grams (like 4-grams), while providing richer context, are more specialized and thus less effective when applied to a different language with potentially different syntactic and morphological patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f20a1c4",
   "metadata": {},
   "source": [
    "#### 5d\n",
    "\n",
    "The words with the lowest perplexities in Dutch and Italian (examples include 'congres', 'rating' and 'international') are influenced by structural simplicity, similarity to English, frequent use in multiple languages, and the context provided by the training data. These factors contribute to the predictability and lower perplexity scores for these words. The effectiveness of 4-gram models and the focus provided by word-level training also play a significant role in achieving lower perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e4abf",
   "metadata": {},
   "source": [
    "#### 5e\n",
    "\n",
    "The words with the highest perplexity in both Italian and Dutch tend to have more complex structures, including characters and accents not found in English ('reëel'). These words often have letter repetitions ('ridurre') and are specific or less frequent in usage ('aangemoedigd'), making them harder to predict for a model trained on English data. The challenges posed by these words are exacerbated in 4-gram models, which, while capturing more context, also demand more specific training data for accurate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
